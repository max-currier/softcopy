{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV text recognition testing\n",
    "\n",
    "In this notebook i am adapting code from [pyimagesearch](https://www.pyimagesearch.com/2018/09/17/opencv-ocr-and-text-recognition-with-tesseract/) to detect and recognize text from images of bookshelves using OpenCV and Tesseract. The goal here it to take a photo of a bookshelf and return a list of books and authors. In the next notebook I will match the list of titles and authors to records in a database of popular books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary packages\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import cv2\n",
    "\n",
    "# cleaning strings\n",
    "import re\n",
    "import string\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# if using on cli\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR functions\n",
    "\n",
    "These are the functions I will use to do OCR on my images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(scores, geometry):\n",
    "    # grab the number of rows and columns from the scores volume, then\n",
    "    # initialize our set of bounding box rectangles and corresponding\n",
    "    # confidence scores\n",
    "    (numRows, numCols) = scores.shape[2:4]\n",
    "    rects = []\n",
    "    confidences = []\n",
    "\n",
    "    # loop over the number of rows\n",
    "    for y in range(0, numRows):\n",
    "        # extract the scores (probabilities), followed by the\n",
    "        # geometrical data used to derive potential bounding box\n",
    "        # coordinates that surround text\n",
    "        scoresData = scores[0, 0, y]\n",
    "        xData0 = geometry[0, 0, y]\n",
    "        xData1 = geometry[0, 1, y]\n",
    "        xData2 = geometry[0, 2, y]\n",
    "        xData3 = geometry[0, 3, y]\n",
    "        anglesData = geometry[0, 4, y]\n",
    "\n",
    "        # loop over the number of columns\n",
    "        for x in range(0, numCols):\n",
    "            # if our score does not have sufficient probability,\n",
    "            # ignore it\n",
    "            if scoresData[x] < args[\"min_confidence\"]:\n",
    "                continue\n",
    "\n",
    "            # compute the offset factor as our resulting feature\n",
    "            # maps will be 4x smaller than the input image\n",
    "            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "\n",
    "            # extract the rotation angle for the prediction and\n",
    "            # then compute the sin and cosine\n",
    "            angle = anglesData[x]\n",
    "            cos = np.cos(angle)\n",
    "            sin = np.sin(angle)\n",
    "\n",
    "            # use the geometry volume to derive the width and height\n",
    "            # of the bounding box\n",
    "            h = xData0[x] + xData2[x]\n",
    "            w = xData1[x] + xData3[x]\n",
    "\n",
    "            # compute both the starting and ending (x, y)-coordinates\n",
    "            # for the text prediction bounding box\n",
    "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "            startX = int(endX - w)\n",
    "            startY = int(endY - h)\n",
    "\n",
    "            # add the bounding box coordinates and probability score\n",
    "            # to our respective lists\n",
    "            rects.append((startX, startY, endX, endY))\n",
    "            confidences.append(scoresData[x])\n",
    "\n",
    "    # return a tuple of the bounding boxes and associated confidences\n",
    "    return (rects, confidences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_text(args):\n",
    "    '''\n",
    "    takes dict of parameters\n",
    "    returns text results from image\n",
    "\n",
    "        resizes, preprocesses images\n",
    "        gets bondary boxes\n",
    "        pulls text\n",
    "    '''\n",
    "    \n",
    "    # load the input image and grab the image dimensions\n",
    "    image = cv2.imread(args[\"image\"])\n",
    "    orig = image.copy()\n",
    "    (origH, origW) = image.shape[:2]\n",
    "\n",
    "    # set the new width and height and then determine the ratio in change\n",
    "    # for both the width and height\n",
    "    (newW, newH) = (args[\"width\"], args[\"height\"])\n",
    "    rW = origW / float(newW)\n",
    "    rH = origH / float(newH)\n",
    "\n",
    "    # resize the image and grab the new image dimensions\n",
    "    image = cv2.resize(image, (newW, newH))\n",
    "    (H, W) = image.shape[:2]\n",
    "    \n",
    "    # additional preprocessing\n",
    "    #image = get_grayscale(image)\n",
    "    \n",
    "    #image = remove_noise(image)\n",
    "\n",
    "    # define the two output layer names for the EAST detector model that\n",
    "    # we are interested -- the first is the output probabilities and the\n",
    "    # second can be used to derive the bounding box coordinates of text\n",
    "    layerNames = [\n",
    "        \"feature_fusion/Conv_7/Sigmoid\",\n",
    "        \"feature_fusion/concat_3\"]\n",
    "\n",
    "    # load the pre-trained EAST text detector\n",
    "    #print(\"[INFO] loading EAST text detector...\")\n",
    "    net = cv2.dnn.readNet(args[\"east\"])\n",
    "\n",
    "    # construct a blob from the image and then perform a forward pass of\n",
    "    # the model to obtain the two output layer sets\n",
    "    blob = cv2.dnn.blobFromImage(image, \n",
    "                                 1.0, \n",
    "                                 (W, H),\n",
    "                                 (123.68, 116.78, 103.94), \n",
    "                                 swapRB=True, \n",
    "                                 crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    (scores, geometry) = net.forward(layerNames)\n",
    "\n",
    "    # decode the predictions, then  apply non-maxima suppression to\n",
    "    # suppress weak, overlapping bounding boxes\n",
    "\n",
    "    (rects, confidences) = decode_predictions(scores, geometry)\n",
    "    boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
    "    \n",
    "    # initialize the list of results\n",
    "    results = []\n",
    "\n",
    "    # loop over the bounding boxes\n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # scale the bounding box coordinates based on the respective\n",
    "        # ratios\n",
    "        startX = int(startX * rW)\n",
    "        startY = int(startY * rH)\n",
    "        endX = int(endX * rW)\n",
    "        endY = int(endY * rH)\n",
    "\n",
    "        # in order to obtain a better OCR of the text we can potentially\n",
    "        # apply a bit of padding surrounding the bounding box -- here we\n",
    "        # are computing the deltas in both the x and y directions\n",
    "        dX = int((endX - startX) * args[\"padding\"])\n",
    "        dY = int((endY - startY) * args[\"padding\"])\n",
    "\n",
    "        # apply padding to each side of the bounding box, respectively\n",
    "        startX = max(0, startX - dX)\n",
    "        startY = max(0, startY - dY)\n",
    "        endX = min(origW, endX + (dX * 2))\n",
    "        endY = min(origH, endY + (dY * 2))\n",
    "\n",
    "        # extract the actual padded ROI\n",
    "        roi = orig[startY:endY, startX:endX]\n",
    "\n",
    "        # in order to apply Tesseract v4 to OCR text we must supply\n",
    "        # (1) a language, (2) an OEM flag of 4, indicating that the we\n",
    "        # wish to use the LSTM neural net model for OCR, and finally\n",
    "        # (3) an OEM value, in this case, 7 which implies that we are\n",
    "        # treating the ROI as a single line of text\n",
    "        config = (\"-l eng --oem 1 --psm 7\")\n",
    "        text = pytesseract.image_to_string(roi, config=config)\n",
    "\n",
    "        # add the bounding box coordinates and OCR'd text to the list\n",
    "        # of results\n",
    "        results.append(((startX, startY, endX, endY), text))\n",
    "        \n",
    "    # sort the results bounding box coordinates from top to bottom\n",
    "    results = sorted(results, key=lambda r: r[0][1])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up strings\n",
    "\n",
    "def clean_string(text):\n",
    "    # clean string to remove non-ASCII text\n",
    "    text = \"\".join([c if ord(c) < 128 else \"\" for c in text])\n",
    "\n",
    "    # standard cleaning\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('[‘’“”…–]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return each horizontal line of text as a dict value\n",
    "\n",
    "def clean_tesseract_output(raw_results, line_buffer):\n",
    "    '''\n",
    "    Function to clean text pulled from images by Tesseract OCR function.\n",
    "    \n",
    "    raw_results = list produced by Tesseract \n",
    "        ex. list of items that look like this: ((73, 49, 426, 114), \"CAPUTO'S\\n\\x0c\")\n",
    "        \n",
    "    line_buffer = int representing amount of space to allow between lines \n",
    "        This may have to be tweaked depending on how close books are to one another in frame.\n",
    "        POTENTIALLY DETERMINE THIS IN FUNCTION? \n",
    "    '''\n",
    "    # initialize empty dict and list\n",
    "    d = {}\n",
    "    startY_points = []\n",
    "\n",
    "    # iterate through each roi\n",
    "    for ((startX, startY, endX, endY), text) in results:\n",
    "        \n",
    "        text = clean_string(text)\n",
    "\n",
    "        # only first pass, where no value in d yet\n",
    "        if not startY_points: \n",
    "            d[startY] = text\n",
    "            startY_points.append(startY)\n",
    "            \n",
    "        else:    \n",
    "            # checking whether text from roi is on same line as last roi (within line_buffer)\n",
    "            if abs(startY - startY_points[-1]) < line_buffer:\n",
    "                 # if so, adding to dict key of that line\n",
    "                d[startY_points[-1]] = ' '.join([d[startY_points[-1]], text]).strip()\n",
    "\n",
    "            else: \n",
    "                # if not, adding to dict key for new line\n",
    "                d[startY] = text\n",
    "                startY_points.append(startY)\n",
    "    \n",
    "    # return output as a list of names\n",
    "    return [val for val in d.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on images\n",
    "\n",
    "### Sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opencv-text-recognition/images/example_01.jpg\n",
      "['oh ok'] \n",
      "\n",
      "opencv-text-recognition/images/example_02.jpg\n",
      "['middleborougch'] \n",
      "\n",
      "opencv-text-recognition/images/example_03.jpg\n",
      "['estate  agents', ' saxons'] \n",
      "\n",
      "opencv-text-recognition/images/example_04.jpg\n",
      "['caputos shop bake'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = {'image': 'opencv-text-recognition/images/example_01.jpg',\n",
    "       'east': 'opencv-text-detection/frozen_east_text_detection.pb',\n",
    "       'min_confidence': 0.5,\n",
    "       'width': 320,\n",
    "       'height': 320,\n",
    "       'padding': 0.1}\n",
    "\n",
    "for i in range(1,5):\n",
    "    args['image'] = 'opencv-text-recognition/images/example_0' + str(i) + '.jpg'\n",
    "    results = get_image_text(args)\n",
    "    final = clean_tesseract_output(results, 10)\n",
    "    print(args['image'])\n",
    "    print(final, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bookshelf test images 1 - from google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't detect titles when text is horizontal. Since this is the default for bookshelf images, i will have to rotate them manually for now, but later will find a way to rotate them automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'image': 'images/bookshelf9.jpg',\n",
    "       'east': 'opencv-text-detection/frozen_east_text_detection.pb',\n",
    "       'min_confidence': 0.5,\n",
    "       'width': 320,\n",
    "       'height': 320,\n",
    "       'padding': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/bookshelf1.jpg\n",
      "['classics'] \n",
      "\n",
      "images/bookshelf2.jpg\n",
      "['dle tds s poeetry sd spensers po ie', 'sherlo conandovle'] \n",
      "\n",
      "images/bookshelf3.jpg\n",
      "['summer ov', 'bdenan once', 'and  for all', 'lem nowinski fs on aa', 'queens  cy  eae ac', 'aie', 'rire ills', 'fe schwab darkersianeo oemagic', 'ane uo ro yinige', 'ae'] \n",
      "\n",
      "images/bookshelf4.jpg\n",
      "['lag', '', 'moe'] \n",
      "\n",
      "images/bookshelf5.jpg\n",
      "['ae'] \n",
      "\n",
      "images/bookshelf6.jpg\n",
      "[] \n",
      "\n",
      "images/bookshelf7.jpg\n",
      "['ca', 'julian  barnes', 'picador lemon tr', 'v irginia eats qv', 'nda panruool', 'lawrenci the rainbow dil', 'tule'] \n",
      "\n",
      "images/bookshelf8.jpg\n",
      "['destruction', ''] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial look at test bookshelf pictures\n",
    "\n",
    "for i in range(1,9):\n",
    "    args['image'] = 'images/bookshelf' + str(i) + '.jpg'\n",
    "    results = get_image_text(args)\n",
    "    final = clean_tesseract_output(results, 10)\n",
    "    print(args['image'])\n",
    "    print(final, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence =  0.0\n",
      "['a of', 'sager', 'fear', 'ag', '', 'ca periential g seon', '', 'ey sian', '', 'i  s ge', '', '', 'squirrel ia', 'chipmunk', 'ly in c  ere', 'ses recs', 'es julian', 'barnes picador lemon tr', 'j table', '', 'shadows seus corey eye ad ee me', 'as ames', 'sais tte ba', 'eto ya ola  ', 'a ', 'ents crea ue', 'r ', 'a  a   oe ole e', ' am er', '', 'f the waves', '', 'v irginia eats qv', '', ' a i ', '', '', 'nda panruool', 'a', '', 'pa', '', 'ww', 'mut s lawrenci', 'the rainbow dil', 'zz pe an', 'as ol', '', 'ap', 'tule', 'spanish i roy ia'] \n",
      "\n",
      "confidence =  0.1111111111111111\n",
      "['ca seon', 'julian  barnes', 'picador lemon tr', '', 'eto ya ola  ', 'v irginia eats qv', 'nda panruool', 'lawrenci the rainbow dil', 'tule'] \n",
      "\n",
      "confidence =  0.2222222222222222\n",
      "['ca seon', 'julian  barnes', 'picador lemon tr', '', 'eto ya ola  ', 'v irginia eats qv', 'nda panruool', 'lawrenci the rainbow dil', 'tule'] \n",
      "\n",
      "confidence =  0.3333333333333333\n",
      "['ca seon', 'julian  barnes', 'picador lemon tr', '', 'v irginia eats qv', 'nda panruool', 'lawrenci the rainbow dil', 'tule'] \n",
      "\n",
      "confidence =  0.4444444444444444\n",
      "['ca', 'julian  barnes', 'picador lemon tr', 'v irginia eats qv', 'nda panruool', 'lawrenci the rainbow dil', 'tule'] \n",
      "\n",
      "confidence =  0.5555555555555556\n",
      "['ca', 'julian  barnes', 'picador lemon tr', 'v irginia eats qv', 'nda panruool', 'lawrenci the rainbow dil', 'tule'] \n",
      "\n",
      "confidence =  0.6666666666666666\n",
      "['ca', 'julian  barnes', 'picador lemon tr', 'v irginia eats qv', 'nda panruool', 'lawrenci the rainbow dil', 'tule'] \n",
      "\n",
      "confidence =  0.7777777777777777\n",
      "['ca', 'julian  barnes', 'picador lemon tr', 'v irginia eats qv', 'nda panruool', 'lawrenci the rainbow dil', 'tule'] \n",
      "\n",
      "confidence =  0.8888888888888888\n",
      "['ca', 'julian  barnes', 'picador lemon tr', 'v irginia eats qv', 'nda panruool', 'lawrenci the rainbow dil', 'tule'] \n",
      "\n",
      "confidence =  1.0\n",
      "[] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing confidence \n",
    "\n",
    "args = {'image': 'images/bookshelf7.jpg',\n",
    "       'east': 'opencv-text-detection/frozen_east_text_detection.pb',\n",
    "       'min_confidence': 0.5,\n",
    "       'width': 320,\n",
    "       'height': 320,\n",
    "       'padding': 0.1}\n",
    "\n",
    "for i in np.linspace(0,1,10):\n",
    "    args['min_confidence'] = i\n",
    "    results = get_image_text(args)\n",
    "    final = clean_tesseract_output(results, 10)\n",
    "    print('confidence = ',args['min_confidence'])\n",
    "    print(final, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding =  0.0\n",
      "['aetoy', 'julian barnes', 'picador lemon', 'virginia woolf', 'andreny fara', 'fawren the rainbow dhl', 'soya iil'] \n",
      "\n",
      "padding =  0.05555555555555555\n",
      "['sey', 'julian barnes', 'picador lemon', 'i ont woolf', 'min parry', 'tlawrenc the rainbow dhl', 'spanish'] \n",
      "\n",
      "padding =  0.1111111111111111\n",
      "['en', 'julian  barnes', 'picador lemon ti', 'es irginia eats ', 'ay pan rol', 'lawrenci the rainbow pl', 'as eg icie'] \n",
      "\n",
      "padding =  0.16666666666666666\n",
      "[' tames re', 'julian n barnes t', 'picador lemon ta', ' v rertats a', 'elon', 'lawrence  the rainbow', 'sulci'] \n",
      "\n",
      "padding =  0.2222222222222222\n",
      "['oe', 'julian b n barnes th', 'picador lemon tab', 'fi on the w', 'aindred harold man', 'jlawrence  the rainbow dhla', 'uc'] \n",
      "\n",
      "padding =  0.2777777777777778\n",
      "['none', 'julian be an barnes th', 'ccador e lemon tabl', 'e on the wave', 'an at aol', 'hlawrence v e the raine dhlay', 'a see ulon a'] \n",
      "\n",
      "padding =  0.3333333333333333\n",
      "['st', 'julian bai an barnes the', 'picador  e lemon table', 'e ens oe', 'yau aone ', 'lawrence th se the rainbow  es aw', 'sy ice'] \n",
      "\n",
      "padding =  0.38888888888888884\n",
      "['canyon', 'julian bar ian barnes the', 'picador i je lemon table', 'e geen soe', '', 'bela rence the ce the rainbow  dhlaw', ' eerily ey'] \n",
      "\n",
      "padding =  0.4444444444444444\n",
      "[' basa', 'julian barr lian barnes', 'picador le ne lemon table', 'e geen ae', '', 'bh lawrence ther ce the rainbow en c lawr', 'me ice'] \n",
      "\n",
      "padding =  0.5\n",
      "['james rese', 'julian barn lian barnes', 'ra  he lemon table', 'e geen ae', '', 'ba laven nls ice the rainbow en  dhlawr', 'sone'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing padding\n",
    "\n",
    "args = {'image': 'images/bookshelf7.jpg',\n",
    "       'east': 'opencv-text-detection/frozen_east_text_detection.pb',\n",
    "       'min_confidence': 0.5,\n",
    "       'width': 320,\n",
    "       'height': 320,\n",
    "       'padding': 0.1}\n",
    "\n",
    "for i in np.linspace(0,.5,10):\n",
    "    args['padding'] = i\n",
    "    results = get_image_text(args)\n",
    "    final = clean_tesseract_output(results, 10)\n",
    "    print('padding = ',args['padding'])\n",
    "    print(final, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bookshelf test images 2 - taken myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gg ll  ns bill wi ea aimle',\n",
       " 'drow diaz junot',\n",
       " 'rule evle hl  history o world gombrich zoe the',\n",
       " 'immortality milan kundera i',\n",
       " 'great gatcrby itzgeral d the',\n",
       " 'narcissus an d goldmund hermann he  hesse fsg fall',\n",
       " 'lhe albert camus']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {'image': 'images/bookshelf9.jpg',\n",
    "       'east': 'opencv-text-detection/frozen_east_text_detection.pb',\n",
    "       'min_confidence': 0.5,\n",
    "       'width': 320,\n",
    "       'height': 320,\n",
    "       'padding': 0.1}\n",
    "\n",
    "# padding = 0.2, buffer = 300, min_confidence = 0.5\n",
    "results = get_image_text(args)\n",
    "final = clean_tesseract_output(results, 300)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is WAY better! Images must do better when clearly framed and maybe even in higher resolution or with images taken up close. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results\n",
    "\n",
    "See bounding box in context of image with OCR results alongside ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR TEXT\n",
      "========\n",
      "oo ts\n",
      "\f",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loop over the results\n",
    "for ((startX, startY, endX, endY), text) in results:\n",
    "    # display the text OCR'd by Tesseract\n",
    "    print(\"OCR TEXT\")\n",
    "    print(\"========\")\n",
    "    print(\"{}\\n\".format(text))\n",
    "\n",
    "    # strip out non-ASCII text so we can draw the text on the image\n",
    "    # using OpenCV, then draw the text and a bounding box surrounding\n",
    "    # the text region of the input image\n",
    "    \n",
    "    text = \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\n",
    "    output = orig.copy()\n",
    "    cv2.rectangle(output, \n",
    "                  (startX, startY), \n",
    "                  (endX, endY),\n",
    "                  (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.putText(output, \n",
    "                text, \n",
    "                (startX, startY - 20),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                1.2, \n",
    "                (0, 0, 255), 3)\n",
    "\n",
    "    # show the output image\n",
    "    cv2.imshow(\"Text Detection\", output)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI Implementation\n",
    "\n",
    "To use this instead of dict of args to run from shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # construct the argument parser and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "\n",
    "# ap.add_argument('-f') # ADDED TO RUN IN NOTEBOOK (argparse is meant for CLI)\n",
    "\n",
    "# ap.add_argument(\"-i\", \n",
    "#                 \"--image\", \n",
    "#                 type=str,\n",
    "#                 help=\"path to input image\")\n",
    "\n",
    "# ap.add_argument(\"-east\", \n",
    "#                 \"--east\", \n",
    "#                 type=str,\n",
    "#                 help=\"path to input EAST text detector\")\n",
    "\n",
    "# ap.add_argument(\"-c\", \n",
    "#                 \"--min-confidence\", \n",
    "#                 type=float, \n",
    "#                 default=0.5,\n",
    "#                 help=\"minimum probability required to inspect a region\")\n",
    "\n",
    "# ap.add_argument(\"-w\", \n",
    "#                 \"--width\", \n",
    "#                 type=int, \n",
    "#                 default=320,\n",
    "#                 help=\"nearest multiple of 32 for resized width\")\n",
    "\n",
    "# ap.add_argument(\"-e\", \n",
    "#                 \"--height\", \n",
    "#                 type=int, \n",
    "#                 default=320,\n",
    "#                 help=\"nearest multiple of 32 for resized height\")\n",
    "\n",
    "# ap.add_argument(\"-p\", \n",
    "#                 \"--padding\", \n",
    "#                 type=float, \n",
    "#                 default=0.0,\n",
    "#                 help=\"amount of padding to add to each border of ROI\")\n",
    "\n",
    "# args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image pre-processing\n",
    "\n",
    "Consider adding these when processing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nanonets.com/blog/ocr-with-tesseract/\n",
    "# get grayscale image\n",
    "def get_grayscale(image):\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY, kernel)\n",
    "\n",
    "# noise removal\n",
    "def remove_noise(image):\n",
    "    return cv2.medianBlur(image,5)\n",
    " \n",
    "#thresholding\n",
    "def thresholding(image):\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "#dilation\n",
    "def dilate(image):\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    return cv2.dilate(image, kernel, iterations = 1)\n",
    "    \n",
    "#erosion\n",
    "def erode(image):\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    return cv2.erode(image, kernel, iterations = 1)\n",
    "\n",
    "#opening - erosion followed by dilation\n",
    "def opening(image):\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "#canny edge detection\n",
    "def canny(image):\n",
    "    return cv2.Canny(image, 100, 200)\n",
    "\n",
    "#skew correction\n",
    "def deskew(image):\n",
    "    coords = np.column_stack(np.where(image > 0))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated\n",
    "\n",
    "#template matching\n",
    "def match_template(image, template):\n",
    "    return cv2.matchTemplate(image, template, cv2.TM_CCOEFF_NORMED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f35173c4ac80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedianBlur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0merosion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdilation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "kernel = np.ones((5,5), np.uint8)\n",
    "noise = cv2.medianBlur(img, 5)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY, kernel)\n",
    "erosion = cv2.erode(gray, kernel, iterations = 1)\n",
    "dilation = cv2.dilate(gray, kernel, iterations = 1)\n",
    "opening = cv2.morphologyEx(gray, cv2.MORPH_OPEN, kernel)\n",
    "closing = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)\n",
    "gradient = cv2.morphologyEx(gray, cv2.MORPH_GRADIENT, kernel)\n",
    "tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, kernel)\n",
    "blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'image': 'images/bookshelf9.jpg',\n",
    "       'east': 'opencv-text-detection/frozen_east_text_detection.pb',\n",
    "       'min_confidence': 0.5,\n",
    "       'width': 320,\n",
    "       'height': 320,\n",
    "       'padding': 0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ga babe  nes bil',\n",
       " 'ess aim',\n",
       " 'drov diaz junot',\n",
       " 'mle tlr listory woo rtt fsombrich of the',\n",
       " 'immortalitv milan kundera',\n",
       " 'great gatery tzgerald the',\n",
       " 'narcissus di  goldmund hermann t hesse',\n",
       " 'fsg',\n",
       " 'fall lhe',\n",
       " 'albert camus']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding = 0.05\n",
    "results = get_image_text(args)\n",
    "final = clean_tesseract_output(results, 25)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ga babe  nes bil ess aim',\n",
       " 'drov diaz junot',\n",
       " 'mle tlr listory woo rtt fsombrich of the',\n",
       " 'immortalitv milan kundera',\n",
       " 'great gatery tzgerald the',\n",
       " 'narcissus di  goldmund hermann t hesse fsg fall',\n",
       " 'lhe albert camus']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
